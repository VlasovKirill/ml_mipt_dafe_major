{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational AutoEncoders\n",
    "\n",
    "Hi! Today we are going to learn about variationals autoencoders. We'll code them to encode handwritten numbers and restore them from the compact vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U catalyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T13:39:13.016056Z",
     "start_time": "2021-05-07T13:39:13.001308Z"
    }
   },
   "outputs": [],
   "source": [
    "from catalyst.utils import set_global_seed, get_device\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import tkinter\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "\n",
    "set_global_seed(42)\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catalyst\n",
    "catalyst.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll work with `MNIST` dataset. Download it, show examples of the writting and prepare the dataset to be loaded into models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T13:34:24.188912Z",
     "start_time": "2021-05-07T13:34:23.886652Z"
    }
   },
   "outputs": [],
   "source": [
    "from catalyst.contrib.datasets import mnist\n",
    "\n",
    "\n",
    "train = mnist.MNIST(\".\", train=True, download=True)\n",
    "valid = mnist.MNIST(\".\", train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T13:34:28.333803Z",
     "start_time": "2021-05-07T13:34:25.129944Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "_, axs = plt.subplots(4, 4, figsize=(6.4 * 1.5, 4.8 * 1.5))\n",
    "\n",
    "for i in range(16):\n",
    "    axs[i // 4][i % 4].imshow(train[100 * i + i][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T14:28:34.924523Z",
     "start_time": "2021-05-07T14:28:34.921525Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T14:28:35.814318Z",
     "start_time": "2021-05-07T14:28:35.805316Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import typing as tp\n",
    "from catalyst.utils import get_loader\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "num_workers = 0\n",
    "\n",
    "\n",
    "def transform(x: np.array) -> tp.Dict[str, torch.Tensor]:\n",
    "    image = torch.FloatTensor(x[\"image\"])\n",
    "    image = ... # Use torch.where, to replace 256 values to ones or zeros\n",
    "    return {'image': image, \"targets\": x[\"targets\"]}\n",
    "\n",
    "\n",
    "train_data_loader = get_loader(\n",
    "    train,\n",
    "    open_fn=lambda x : {'image': x[0].reshape(1, 28, 28), 'targets': x[1]},\n",
    "    dict_transform=transform,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    shuffle=True,\n",
    "    sampler=None,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "valid_data_loader = get_loader(\n",
    "    valid,\n",
    "    open_fn=lambda x : {'image': x[0].reshape(1, 28, 28), 'targets': x[1]},\n",
    "    dict_transform=transform,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    shuffle=False,\n",
    "    sampler=None,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T14:28:37.573863Z",
     "start_time": "2021-05-07T14:28:37.535854Z"
    }
   },
   "outputs": [],
   "source": [
    "next(iter(valid_data_loader))['image'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variational autoencoder consists of two parts: encoder and decoder. The encoder shrinks objects into some vector. The decoder generates an proximate an \"image\" of object. In our case, objects are images. We will use CNNs for encoding images and UpScale Convolution operations for decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T14:08:50.411463Z",
     "start_time": "2021-05-07T14:08:50.395201Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_size: int = 2):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create encoder model!\n",
    "        self.feature_extractor = ...\n",
    "        self.latent_space = ...\n",
    "        \n",
    "        self.latent_size = latent_size\n",
    "        \n",
    "    def forward(self, images: torch.Tensor) -> tp.Tuple[torch.Tensor, torch.Tensor]:\n",
    "        features = self.feature_extractor(images)\n",
    "        latent = self.latent_space(features)\n",
    "        return latent[:, :self.latent_size], latent[:, self.latent_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T14:08:52.263411Z",
     "start_time": "2021-05-07T14:08:52.237548Z"
    }
   },
   "outputs": [],
   "source": [
    "from catalyst.contrib.nn.modules import Lambda\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size: tp.Tuple[int, int] = (28, 28),\n",
    "        latent_size: int = 2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.image_size = image_size\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        # Create Decoder model!\n",
    "        self.map_generator = ...\n",
    "        self.deconv = ...\n",
    "        self.output = ...\n",
    "            \n",
    "    def forward(self, points: torch.Tensor) -> torch.Tensor:\n",
    "        feature_map = self.map_generator(points)\n",
    "        feature_map = self.deconv(feature_map)\n",
    "        return self.output(feature_map)\n",
    "            \n",
    "    def make_up_layer_(self, in_channels: int, out_channels: int) -> torch.Tensor:\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joint the encoder and decoder to create VAE! We have discussed in the lecture about it, and we knew how to train VAE. We need sample points in latent space, pass them forward through the decoder and compare a decoder result with original object. Also we should sample points from some normal distribution, which parameters approach to $(0, I)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T14:08:52.582685Z",
     "start_time": "2021-05-07T14:08:52.558656Z"
    }
   },
   "outputs": [],
   "source": [
    "LOG_SCALE_MAX = 2\n",
    "LOG_SCALE_MIN = -10\n",
    "\n",
    "def normal_sample(loc: torch.Tensor, log_scale: torch.Tensor) -> torch.Tensor:\n",
    "    scale = torch.exp(0.5 * log_scale)\n",
    "    return loc + scale * torch.randn_like(scale)\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_size: tp.Tuple[int, int] = (28, 28), latent_size: int = 2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(latent_size)\n",
    "        self.decoder = Decoder(image_size, latent_size)\n",
    "        \n",
    "    def forward(self, images: torch.Tensor) -> tp.Dict[str, torch.Tensor]:\n",
    "        loc, log_scale = ... # get loc and scale for sampling\n",
    "        log_scale = torch.clamp(log_scale, LOG_SCALE_MIN, LOG_SCALE_MAX)\n",
    "\n",
    "        z_ = normal_sample(loc, log_scale) if self.training else loc\n",
    "        x_ = ... # recreate object from z_\n",
    "\n",
    "        return {\n",
    "            \"decoder_result\": x_,\n",
    "            \"loc\": loc,\n",
    "            \"log_scale\": log_scale\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T14:08:52.752511Z",
     "start_time": "2021-05-07T14:08:52.729621Z"
    }
   },
   "outputs": [],
   "source": [
    "class KLVAELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, loc: torch.Tensor, log_scale: torch.Tensor) -> torch.Tensor:\n",
    "        return (-0.5 * torch.sum(1 + log_scale - loc.pow(2) - log_scale.exp(), dim=1)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to modify `BinaryCrossEntropyLoss` function, because it doesn't work properly with images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T14:08:53.206338Z",
     "start_time": "2021-05-07T14:08:53.189657Z"
    }
   },
   "outputs": [],
   "source": [
    "class ImageCELoss(nn.BCEWithLogitsLoss):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        input = input.view(-1) # reshape input to (batch_size * ...)\n",
    "        target = target.view(-1) # reshape target to (batch_size * ...) [bs, C_0, .., C_n]\n",
    "        return super().forward(input, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To monitor decoded images, we have to write a new callback function. It will log image into the tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T15:01:56.853301Z",
     "start_time": "2021-05-07T15:01:56.849116Z"
    }
   },
   "outputs": [],
   "source": [
    "from catalyst import dl\n",
    "from catalyst.core import Callback, CallbackOrder\n",
    "\n",
    "\n",
    "class LogFigureCallback(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__(CallbackOrder.External)\n",
    "\n",
    "    def on_epoch_end(self, runner: dl.Runner):\n",
    "        if runner.is_valid_loader:\n",
    "            tb_callback = runner.loggers[\"tensorboard\"]\n",
    "            logger = tb_callback.loggers[runner.loader_key]\n",
    "            decoder_result = runner.output[\"decoder_result\"]\n",
    "            logger.add_images(\n",
    "                \"image/epoch\", \n",
    "                torch.sigmoid(decoder_result), # create image from decoder result\n",
    "#                 global_step=runner.epoch,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model, criterion, optimizer. Train model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T15:01:57.753333Z",
     "start_time": "2021-05-07T15:01:57.729761Z"
    }
   },
   "outputs": [],
   "source": [
    "from catalyst.contrib.nn.optimizers import RAdam\n",
    "\n",
    "\n",
    "model = VAE()\n",
    "criterion = {\n",
    "    \"ae\": ImageCELoss(),\n",
    "    \"kl\": KLVAELoss()\n",
    "}\n",
    "optimizer = RAdam(model.parameters(), lr=1e-2)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T15:01:58.020378Z",
     "start_time": "2021-05-07T15:01:57.975335Z"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.ones((256, 1, 28, 28))\n",
    "\n",
    "out = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T15:01:58.302923Z",
     "start_time": "2021-05-07T15:01:58.292383Z"
    }
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    dl.CriterionCallback(\n",
    "        input_key=\"decoder_result\", target_key=\"features\", metric_key=\"loss_ae\", criterion_key=\"ae\",\n",
    "    ),\n",
    "    dl.CriterionCallback(\n",
    "        input_key='log_scale', target_key=\"loc\", metric_key=\"loss_kl\", criterion_key=\"kl\"\n",
    "    ),\n",
    "    dl.MetricAggregationCallback(\n",
    "        metric_key=\"loss\",\n",
    "        mode=\"weighted_sum\",\n",
    "        metrics={\"loss_ae\": 1.0, \"loss_kl\": 0.01},\n",
    "    ),\n",
    "    LogFigureCallback(),\n",
    "    dl.SchedulerCallback(),\n",
    "    dl.CheckpointCallback(logdir=Path(\"logs\") / datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "                          loader_key=\"valid\", metric_key=\"loss\", minimize=True),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T15:01:59.696389Z",
     "start_time": "2021-05-07T15:01:59.685874Z"
    }
   },
   "outputs": [],
   "source": [
    "class VAERunner(dl.SupervisedRunner):\n",
    "    def predict_batch(self, batch: tp.Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        predict  = self.model(batch[\"image\"].to(self.device))\n",
    "        prediction = {\n",
    "            'features': batch[\"image\"],\n",
    "            \"targets\": batch[\"targets\"],\n",
    "            'decoder_result': predict['decoder_result'],\n",
    "            'loc': predict['loc'],\n",
    "            'log_scale': predict['log_scale'],\n",
    "            'logits': predict['logits']\n",
    "        }\n",
    "        return prediction\n",
    "    \n",
    "    def handle_batch(self, batch: tp.Dict[str, torch.Tensor]):\n",
    "        self.output = self.model(batch[\"image\"])\n",
    "        \n",
    "        self.batch = {\n",
    "            'features': batch[\"image\"],\n",
    "            \"targets\": batch[\"targets\"],\n",
    "            'decoder_result': self.output['decoder_result'],\n",
    "            'loc': self.output['loc'],\n",
    "            'log_scale': self.output['log_scale'],\n",
    "        }\n",
    "        \n",
    "runner = VAERunner(input_key='images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T15:02:02.625746Z",
     "start_time": "2021-05-07T15:02:02.599747Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "logdir = Path(\"logs\") / datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T15:04:53.499084Z",
     "start_time": "2021-05-07T15:02:09.021667Z"
    }
   },
   "outputs": [],
   "source": [
    "runner.train(\n",
    "    engine=dl.DeviceEngine(device),\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    scheduler=scheduler,\n",
    "    loggers={\"tensorboard\": dl.TensorboardLogger(logdir=logdir)},\n",
    "    loaders={\"train\": train_data_loader, \"valid\": valid_data_loader},\n",
    "    callbacks=callbacks,\n",
    "    num_epochs=1,\n",
    "    logdir=logdir,\n",
    "    load_best_on_end=True,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main feature of VAE it's a generating new objects. We can do this by mixing latent representation of objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T15:04:58.140316Z",
     "start_time": "2021-05-07T15:04:58.107251Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data = next(iter(valid_data_loader))\n",
    "test_data[\"targets\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T15:06:04.538781Z",
     "start_time": "2021-05-07T15:06:04.518221Z"
    }
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "locs, _ = ... # get model prediction on test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_transition(i: int, j: int):\n",
    "    _, ax = plt.subplots(1, 11, figsize=(15, 5))\n",
    "    \n",
    "    line = np.linspace(0, 1, 11)\n",
    "    for k in range(0, 11):\n",
    "        point = line[k] * locs[j] + (1 - line[k]) * locs[i]\n",
    "        decoded = ... # create image from point\n",
    "        ax[k].imshow(...) # plot decoded!\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_transition(0, -3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can enhance generated images by many ways. And we choose to add classification task. The model will classify object based on the corresponding latent representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEClassify(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int = 10,\n",
    "        image_size: tp.Tuple[int, int] = (28, 28),\n",
    "        latent_size: int = 10,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(latent_size)\n",
    "        self.decoder = Decoder(image_size, latent_size)\n",
    "        self.clf = ...\n",
    "        \n",
    "    def forward(self, images: torch.Tensor) -> tp.Dict[str, torch.Tensor]:\n",
    "        loc, log_scale = self.encoder(images)\n",
    "        log_scale = torch.clamp(log_scale, LOG_SCALE_MIN, LOG_SCALE_MAX)\n",
    "\n",
    "        z_ = normal_sample(loc, log_scale) if self.training else loc\n",
    "        x_ = self.decoder(z_)\n",
    "\n",
    "        logits = self.clf(z_)\n",
    "        return {\n",
    "            \"logits\": logits, \n",
    "            \"decoder_result\": x_,\n",
    "            \"loc\": loc,\n",
    "            \"log_scale\": log_scale\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.contrib.nn.optimizers import RAdam\n",
    "\n",
    "\n",
    "model = VAEClassify()\n",
    "criterion = {\n",
    "    \"ce\": nn.CrossEntropyLoss(),\n",
    "    \"ae\": ImageCELoss(),\n",
    "    \"kl\": KLVAELoss()\n",
    "}\n",
    "optimizer = RAdam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    dl.CriterionCallback(\n",
    "        input_key=\"decoder_result\", target_key=\"features\", metric_key=\"loss_ae\", criterion_key=\"ae\",\n",
    "    ),\n",
    "    dl.CriterionCallback(\n",
    "        input_key='log_scale', target_key=\"loc\", metric_key=\"loss_kl\", criterion_key=\"kl\"\n",
    "    ),\n",
    "    dl.CriterionCallback(\n",
    "        input_key=\"logits\", target_key=\"targets\", metric_key=\"loss_ce\", criterion_key=\"ce\",\n",
    "    ),\n",
    "    dl.MetricAggregationCallback(\n",
    "        metric_key=\"loss\",\n",
    "        mode=\"weighted_sum\",\n",
    "        metrics={\"loss_ae\": 1.0, \"loss_kl\": 0.01, \"loss_ce\": 1.0},\n",
    "    ),\n",
    "    dl.AccuracyCallback(input_key=\"logits\", target_key=\"targets\"),\n",
    "    LogFigureCallback(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAERunner(dl.SupervisedRunner):\n",
    "    def predict_batch(self, batch: tp.Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        predict  = self.model(batch[\"image\"].to(self.device))\n",
    "        prediction = {\n",
    "            'features': batch[\"image\"],\n",
    "            \"targets\": batch[\"targets\"],\n",
    "            'decoder_result': predict['decoder_result'],\n",
    "            'loc': predict['loc'],\n",
    "            'log_scale': predict['log_scale'],\n",
    "            'logits': predict['logits']\n",
    "        }\n",
    "        return prediction\n",
    "    \n",
    "    def handle_batch(self, batch: tp.Dict[str, torch.Tensor]):\n",
    "        self.output = self.model(batch[\"image\"])\n",
    "        \n",
    "        self.batch = {\n",
    "            'features': batch[\"image\"],\n",
    "            \"targets\": batch[\"targets\"],\n",
    "            'decoder_result': self.output['decoder_result'],\n",
    "            'loc': self.output['loc'],\n",
    "            'log_scale': self.output['log_scale'],\n",
    "            'logits': self.output['logits']\n",
    "        }\n",
    "        \n",
    "runner = VAERunner(input_key='images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.train(\n",
    "    engine=dl.DeviceEngine(device),\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    scheduler=scheduler,\n",
    "    loggers={\"tensorboard\": dl.TensorboardLogger(logdir=logdir)},\n",
    "    loaders={\"train\": train_data_loader, \"valid\": valid_data_loader},\n",
    "    callbacks=callbacks,\n",
    "    num_epochs=1,\n",
    "    logdir=logdir,\n",
    "    load_best_on_end=True,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare results with the usual VAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "locs, _ = model.encoder(test_data[\"image\"].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_transition(0, -3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how our model restore noised objects. The model aren't trained to restore, but it can do this very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(2, 6, figsize=(10, 4))\n",
    "    \n",
    "for k in range(0, 12):\n",
    "    image = test_data[\"image\"][k]\n",
    "    ax[k // 6][k % 6].imshow(image.squeeze().cpu().detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(2, 6, figsize=(10, 4))\n",
    "    \n",
    "for k in range(0, 12):\n",
    "    image = test_data[\"image\"][k]\n",
    "    noise = ... # let's make some noise\n",
    "    ax[k // 6][k % 6].imshow((image + noise).squeeze().cpu().detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(2, 6, figsize=(10, 4))\n",
    "    \n",
    "for k in range(0, 12):\n",
    "    image = test_data[\"image\"][k]\n",
    "    noise = ... # let's make some noise\n",
    "    point, _ = ... # get noised objects vector representation from latent space\n",
    "    decoded = ... # decode points\n",
    "    ax[k // 6][k % 6].imshow(...) # plot decoded\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, let's look at the latent space. We choose 2D plain space, so it's easy to plot the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {\"image\": [], \"loc\": [], \"target\": []}\n",
    "\n",
    "for pred in runner.predict_loader(loader=valid_data_loader):\n",
    "    # Put predicted loc and targets from pred into predictions\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[\"x\"] = [o[0] for o in predictions[\"loc\"]]\n",
    "predictions[\"y\"] = [o[1] for o in predictions[\"loc\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "\n",
    "# Use sns.scatterplot to show points !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
