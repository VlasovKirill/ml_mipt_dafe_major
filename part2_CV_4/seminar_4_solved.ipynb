{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational AutoEncoders\n",
    "\n",
    "Hi! Today we are going to learn about variationals autoencoders. We'll code them to encode handwritten numbers and restore them from the compact vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U catalyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T13:39:13.016056Z",
     "start_time": "2021-05-07T13:39:13.001308Z"
    }
   },
   "outputs": [],
   "source": [
    "from catalyst.utils import set_global_seed, get_device\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import tkinter\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "\n",
    "set_global_seed(42)\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'21.04.2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import catalyst\n",
    "catalyst.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll work with `MNIST` dataset. Download it, show examples of the writting and prepare the dataset to be loaded into models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T13:34:24.188912Z",
     "start_time": "2021-05-07T13:34:23.886652Z"
    }
   },
   "outputs": [],
   "source": [
    "from catalyst.contrib.datasets import mnist\n",
    "\n",
    "\n",
    "train = mnist.MNIST(\".\", train=True, download=True)\n",
    "valid = mnist.MNIST(\".\", train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T13:34:28.333803Z",
     "start_time": "2021-05-07T13:34:25.129944Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "_, axs = plt.subplots(4, 4, figsize=(6.4 * 1.5, 4.8 * 1.5))\n",
    "\n",
    "for i in range(16):\n",
    "    axs[i // 4][i % 4].imshow(train[100 * i + i][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T14:28:34.924523Z",
     "start_time": "2021-05-07T14:28:34.921525Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T14:28:35.814318Z",
     "start_time": "2021-05-07T14:28:35.805316Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import typing as tp\n",
    "from catalyst.utils import get_loader\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "num_workers = 0\n",
    "\n",
    "\n",
    "def transform(x: np.array) -> tp.Dict[str, torch.Tensor]:\n",
    "    image = torch.FloatTensor(x[\"image\"])\n",
    "    image = torch.where(image > 127, 1, 0).float() # Use torch.where, to replace 256 values to ones or zeros\n",
    "    return {'image': image, \"targets\": x[\"targets\"]}\n",
    "\n",
    "\n",
    "train_data_loader = get_loader(\n",
    "    train,\n",
    "    open_fn=lambda x : {'image': x[0].reshape(1, 28, 28), 'targets': x[1]},\n",
    "    dict_transform=transform,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    shuffle=True,\n",
    "    sampler=None,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "valid_data_loader = get_loader(\n",
    "    valid,\n",
    "    open_fn=lambda x : {'image': x[0].reshape(1, 28, 28), 'targets': x[1]},\n",
    "    dict_transform=transform,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    shuffle=False,\n",
    "    sampler=None,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T14:28:37.573863Z",
     "start_time": "2021-05-07T14:28:37.535854Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 1, 28, 28])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(valid_data_loader))['image'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variational autoencoder consists of two parts: encoder and decoder. The encoder shrinks objects into some vector. The decoder generates an proximate an \"image\" of object. In our case, objects are images. We will use CNNs for encoding images and UpScale Convolution operations for decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T14:08:50.411463Z",
     "start_time": "2021-05-07T14:08:50.395201Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_size: int = 2):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create encoder model!\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(1, 4, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(4, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool2d((1, 1)),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.latent_space = nn.Linear(64, 2 * latent_size)\n",
    "        \n",
    "        self.latent_size = latent_size\n",
    "        \n",
    "    def forward(self, images: torch.Tensor) -> tp.Tuple[torch.Tensor, torch.Tensor]:\n",
    "        features = self.feature_extractor(images)\n",
    "        latent = self.latent_space(features)\n",
    "        return latent[:, :self.latent_size], latent[:, self.latent_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T14:08:52.263411Z",
     "start_time": "2021-05-07T14:08:52.237548Z"
    }
   },
   "outputs": [],
   "source": [
    "from catalyst.contrib.nn.modules import Lambda\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size: tp.Tuple[int, int] = (28, 28),\n",
    "        latent_size: int = 2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.image_size = image_size\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        # Create Decoder model!\n",
    "        self.map_generator = nn.Sequential(\n",
    "            nn.Linear(latent_size, 64 * 49),\n",
    "            Lambda(lambda x: x.view(x.size(0), 64, 7, 7)),\n",
    "        )\n",
    "        self.deconv = nn.Sequential(\n",
    "            self.make_up_layer_(64, 16), # 7 -> 14\n",
    "            self.make_up_layer_(16, 4), # 14 -> 28\n",
    "        )\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Conv2d(4, 1, 3, padding=1),\n",
    "        )\n",
    "            \n",
    "    def forward(self, points: torch.Tensor) -> torch.Tensor:\n",
    "        feature_map = self.map_generator(points)\n",
    "        feature_map = self.deconv(feature_map)\n",
    "        return self.output(feature_map)\n",
    "            \n",
    "    def make_up_layer_(self, in_channels: int, out_channels: int) -> torch.Tensor:\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, \n",
    "                               kernel_size=3, padding=1,\n",
    "                               output_padding=1, stride=2),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.LeakyReLU(0.01)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joint the encoder and decoder to create VAE! We have discussed in the lecture about it, and we knew how to train VAE. We need sample points in latent space, pass them forward through the decoder and compare a decoder result with original object. Also we should sample points from some normal distribution, which parameters approach to $(0, I)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T14:08:52.582685Z",
     "start_time": "2021-05-07T14:08:52.558656Z"
    }
   },
   "outputs": [],
   "source": [
    "LOG_SCALE_MAX = 2\n",
    "LOG_SCALE_MIN = -10\n",
    "\n",
    "def normal_sample(loc: torch.Tensor, log_scale: torch.Tensor) -> torch.Tensor:\n",
    "    scale = torch.exp(0.5 * log_scale)\n",
    "    return loc + scale * torch.randn_like(scale)\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_size: tp.Tuple[int, int] = (28, 28), latent_size: int = 2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(latent_size)\n",
    "        self.decoder = Decoder(image_size, latent_size)\n",
    "        \n",
    "    def forward(self, images: torch.Tensor) -> tp.Dict[str, torch.Tensor]:\n",
    "        loc, log_scale = self.encoder(images) # get loc and scale for sampling\n",
    "        log_scale = torch.clamp(log_scale, LOG_SCALE_MIN, LOG_SCALE_MAX)\n",
    "\n",
    "        z_ = normal_sample(loc, log_scale) if self.training else loc\n",
    "        x_ = self.decoder(z_) # recreate object from z_\n",
    "\n",
    "        return {\n",
    "            \"decoder_result\": x_,\n",
    "            \"loc\": loc,\n",
    "            \"log_scale\": log_scale\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T14:08:52.752511Z",
     "start_time": "2021-05-07T14:08:52.729621Z"
    }
   },
   "outputs": [],
   "source": [
    "class KLVAELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, loc: torch.Tensor, log_scale: torch.Tensor) -> torch.Tensor:\n",
    "        return (-0.5 * torch.sum(1 + log_scale - loc.pow(2) - log_scale.exp(), dim=1)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to modify `BinaryCrossEntropyLoss` function, because it doesn't work properly with images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T14:08:53.206338Z",
     "start_time": "2021-05-07T14:08:53.189657Z"
    }
   },
   "outputs": [],
   "source": [
    "class ImageCELoss(nn.BCEWithLogitsLoss):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        input = input.view(-1) # reshape input to (batch_size * ...)\n",
    "        target = target.view(-1) # reshape target to (batch_size * ...) [bs, C_0, .., C_n]\n",
    "        return super().forward(input, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To monitor decoded images, we have to write a new callback function. It will log image into the tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T15:01:56.853301Z",
     "start_time": "2021-05-07T15:01:56.849116Z"
    }
   },
   "outputs": [],
   "source": [
    "from catalyst import dl\n",
    "from catalyst.core import Callback, CallbackOrder\n",
    "\n",
    "\n",
    "class LogFigureCallback(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__(CallbackOrder.External)\n",
    "\n",
    "    def on_epoch_end(self, runner: dl.Runner):\n",
    "        if runner.is_valid_loader:\n",
    "            tb_callback = runner.loggers[\"tensorboard\"]\n",
    "            logger = tb_callback.loggers[runner.loader_key]\n",
    "            decoder_result = runner.output[\"decoder_result\"]\n",
    "            logger.add_images(\n",
    "                \"image/epoch\", \n",
    "                torch.sigmoid(decoder_result), # create image from decoder result\n",
    "#                 global_step=runner.epoch,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model, criterion, optimizer. Train model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T15:01:57.753333Z",
     "start_time": "2021-05-07T15:01:57.729761Z"
    }
   },
   "outputs": [],
   "source": [
    "from catalyst.contrib.nn.optimizers import RAdam\n",
    "\n",
    "\n",
    "model = VAE()\n",
    "criterion = {\n",
    "    \"ae\": ImageCELoss(),\n",
    "    \"kl\": KLVAELoss()\n",
    "}\n",
    "optimizer = RAdam(model.parameters(), lr=1e-2)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T15:01:58.020378Z",
     "start_time": "2021-05-07T15:01:57.975335Z"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.ones((256, 1, 28, 28))\n",
    "\n",
    "out = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T15:01:58.302923Z",
     "start_time": "2021-05-07T15:01:58.292383Z"
    }
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    dl.CriterionCallback(\n",
    "        input_key=\"decoder_result\", target_key=\"features\", metric_key=\"loss_ae\", criterion_key=\"ae\",\n",
    "    ),\n",
    "    dl.CriterionCallback(\n",
    "        input_key='log_scale', target_key=\"loc\", metric_key=\"loss_kl\", criterion_key=\"kl\"\n",
    "    ),\n",
    "    dl.MetricAggregationCallback(\n",
    "        metric_key=\"loss\",\n",
    "        mode=\"weighted_sum\",\n",
    "        metrics={\"loss_ae\": 1.0, \"loss_kl\": 0.01},\n",
    "    ),\n",
    "    LogFigureCallback(),\n",
    "    dl.SchedulerCallback(),\n",
    "    dl.CheckpointCallback(logdir=Path(\"logs\") / datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "                          loader_key=\"valid\", metric_key=\"loss\", minimize=True),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T15:01:59.696389Z",
     "start_time": "2021-05-07T15:01:59.685874Z"
    }
   },
   "outputs": [],
   "source": [
    "class VAERunner(dl.SupervisedRunner):\n",
    "    def predict_batch(self, batch: tp.Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        prediction = {\"features\": batch[\"image\"], \"targets\": batch[\"targets\"]}\n",
    "        prediction.update(self.model(batch[\"image\"]))\n",
    "        return prediction\n",
    "    \n",
    "    def handle_batch(self, batch: tp.Dict[str, torch.Tensor]):\n",
    "        self.output = self.model(batch[\"image\"])\n",
    "        \n",
    "        self.batch = {\n",
    "            'features': batch[\"image\"],\n",
    "            \"targets\": batch[\"targets\"],\n",
    "            'decoder_result': self.output['decoder_result'],\n",
    "            'loc': self.output['loc'],\n",
    "            'log_scale': self.output['log_scale'],\n",
    "        }\n",
    "        \n",
    "runner = VAERunner(input_key='images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T15:02:02.625746Z",
     "start_time": "2021-05-07T15:02:02.599747Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 14820), started 0:38:39 ago. (Use '!kill 14820' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d6a74dbe3e572e0f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d6a74dbe3e572e0f\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logdir = Path(\"logs\") / datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T15:04:53.499084Z",
     "start_time": "2021-05-07T15:02:09.021667Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9f36e5532da472f8b102d585fcf4cad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1/1 * Epoch (train):   0%|          | 0/234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (1/1) loss: 0.3507111072540283 | loss_ae: 0.3441738784313202 | loss_ae/mean: 0.3441738784313202 | loss_ae/std: 0.14102233656137952 | loss_kl: 0.6537229418754578 | loss_kl/mean: 0.6537229418754578 | loss_kl/std: 1.8657114680712537 | lr: 0.01 | momentum: 0.9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "676874d6b3984e29a71272dc6bd837d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1/1 * Epoch (valid):   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid (1/1) loss: 0.23700270056724548 | loss_ae: 0.2275197058916092 | loss_ae/mean: 0.2275197058916092 | loss_ae/std: 0.010229723740033607 | loss_kl: 0.9483001232147217 | loss_kl/mean: 0.9483001232147217 | loss_kl/std: 0.07174206305925224 | lr: 0.01 | momentum: 0.9\n",
      "* Epoch (1/1) lr: 0.01 | momentum: 0.9\n",
      "Top best models:\n",
      "logs\\20210511-132850/train.1.pth\t0.2370\n"
     ]
    }
   ],
   "source": [
    "runner.train(\n",
    "    engine=dl.DeviceEngine(device),\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    scheduler=scheduler,\n",
    "    loggers={\"tensorboard\": dl.TensorboardLogger(logdir=logdir)},\n",
    "    loaders={\"train\": train_data_loader, \"valid\": valid_data_loader},\n",
    "    callbacks=callbacks,\n",
    "    num_epochs=1,\n",
    "    logdir=logdir,\n",
    "    load_best_on_end=True,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main feature of VAE it's a generating new objects. We can do this by mixing latent representation of objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T15:04:58.140316Z",
     "start_time": "2021-05-07T15:04:58.107251Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6, 6, 5,\n",
       "        4, 0, 7, 4, 0, 1, 3, 1, 3, 4, 7, 2, 7, 1, 2, 1, 1, 7, 4, 2, 3, 5, 1, 2,\n",
       "        4, 4, 6, 3, 5, 5, 6, 0, 4, 1, 9, 5, 7, 8, 9, 3, 7, 4, 6, 4, 3, 0, 7, 0,\n",
       "        2, 9, 1, 7, 3, 2, 9, 7, 7, 6, 2, 7, 8, 4, 7, 3, 6, 1, 3, 6, 9, 3, 1, 4,\n",
       "        1, 7, 6, 9, 6, 0, 5, 4, 9, 9, 2, 1, 9, 4, 8, 7, 3, 9, 7, 4, 4, 4, 9, 2,\n",
       "        5, 4, 7, 6, 7, 9, 0, 5, 8, 5, 6, 6, 5, 7, 8, 1, 0, 1, 6, 4, 6, 7, 3, 1,\n",
       "        7, 1, 8, 2, 0, 2, 9, 9, 5, 5, 1, 5, 6, 0, 3, 4, 4, 6, 5, 4, 6, 5, 4, 5,\n",
       "        1, 4, 4, 7, 2, 3, 2, 7, 1, 8, 1, 8, 1, 8, 5, 0, 8, 9, 2, 5, 0, 1, 1, 1,\n",
       "        0, 9, 0, 3, 1, 6, 4, 2, 3, 6, 1, 1, 1, 3, 9, 5, 2, 9, 4, 5, 9, 3, 9, 0,\n",
       "        3, 6, 5, 5, 7, 2, 2, 7, 1, 2, 8, 4, 1, 7, 3, 3, 8, 8, 7, 9, 2, 2, 4, 1,\n",
       "        5, 9, 8, 7, 2, 3, 0, 4, 4, 2, 4, 1, 9, 5, 7, 7])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = next(iter(valid_data_loader))\n",
    "test_data[\"targets\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T15:06:04.538781Z",
     "start_time": "2021-05-07T15:06:04.518221Z"
    }
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "locs, _ = model.encoder(test_data[\"image\"].to(device)) # get model prediction on test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_transition(i: int, j: int):\n",
    "    _, ax = plt.subplots(1, 11, figsize=(15, 5))\n",
    "    \n",
    "    line = np.linspace(0, 1, 11)\n",
    "    for k in range(0, 11):\n",
    "        point = line[k] * locs[j] + (1 - line[k]) * locs[i]\n",
    "        decoded = model.decoder(point.unsqueeze(0).to(device)).squeeze() # create image from point\n",
    "        ax[k].imshow(torch.sigmoid(decoded).detach().cpu().numpy().squeeze()) # plot decoded!\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_transition(0, -3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can enhance generated images by many ways. And we choose to add classification task. The model will classify object based on the corresponding latent representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEClassify(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int = 10,\n",
    "        image_size: tp.Tuple[int, int] = (28, 28),\n",
    "        latent_size: int = 10,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(latent_size)\n",
    "        self.decoder = Decoder(image_size, latent_size)\n",
    "        self.clf = nn.Linear(latent_size, num_classes)\n",
    "        \n",
    "    def forward(self, images: torch.Tensor) -> tp.Dict[str, torch.Tensor]:\n",
    "        loc, log_scale = self.encoder(images)\n",
    "        log_scale = torch.clamp(log_scale, LOG_SCALE_MIN, LOG_SCALE_MAX)\n",
    "\n",
    "        z_ = normal_sample(loc, log_scale) if self.training else loc\n",
    "        x_ = self.decoder(z_)\n",
    "\n",
    "        logits = self.clf(z_)\n",
    "        return {\n",
    "            \"logits\": logits, \n",
    "            \"decoder_result\": x_,\n",
    "            \"loc\": loc,\n",
    "            \"log_scale\": log_scale\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.contrib.nn.optimizers import RAdam\n",
    "\n",
    "\n",
    "model = VAEClassify()\n",
    "criterion = {\n",
    "    \"ce\": nn.CrossEntropyLoss(),\n",
    "    \"ae\": ImageCELoss(),\n",
    "    \"kl\": KLVAELoss()\n",
    "}\n",
    "optimizer = RAdam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    dl.CriterionCallback(\n",
    "        input_key=\"decoder_result\", target_key=\"features\", metric_key=\"loss_ae\", criterion_key=\"ae\",\n",
    "    ),\n",
    "    dl.CriterionCallback(\n",
    "        input_key='log_scale', target_key=\"loc\", metric_key=\"loss_kl\", criterion_key=\"kl\"\n",
    "    ),\n",
    "    dl.CriterionCallback(\n",
    "        input_key=\"logits\", target_key=\"targets\", metric_key=\"loss_ce\", criterion_key=\"ce\",\n",
    "    ),\n",
    "    dl.MetricAggregationCallback(\n",
    "        metric_key=\"loss\",\n",
    "        mode=\"weighted_sum\",\n",
    "        metrics={\"loss_ae\": 1.0, \"loss_kl\": 0.01, \"loss_ce\": 1.0},\n",
    "    ),\n",
    "    dl.AccuracyCallback(input_key=\"logits\", target_key=\"targets\"),\n",
    "    LogFigureCallback(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAERunner(dl.SupervisedRunner):\n",
    "    def predict_batch(self, batch: tp.Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        predict  = self.model(batch[\"image\"].to(self.device))\n",
    "        prediction = {\n",
    "            'features': batch[\"image\"],\n",
    "            \"targets\": batch[\"targets\"],\n",
    "            'decoder_result': predict['decoder_result'],\n",
    "            'loc': predict['loc'],\n",
    "            'log_scale': predict['log_scale'],\n",
    "            'logits': predict['logits']\n",
    "        }\n",
    "        return prediction\n",
    "    \n",
    "    def handle_batch(self, batch: tp.Dict[str, torch.Tensor]):\n",
    "        self.output = self.model(batch[\"image\"])\n",
    "        \n",
    "        self.batch = {\n",
    "            'features': batch[\"image\"],\n",
    "            \"targets\": batch[\"targets\"],\n",
    "            'decoder_result': self.output['decoder_result'],\n",
    "            'loc': self.output['loc'],\n",
    "            'log_scale': self.output['log_scale'],\n",
    "            'logits': self.output['logits']\n",
    "        }\n",
    "        \n",
    "runner = VAERunner(input_key='images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6851574162a448e49c44a21c3412480c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1/1 * Epoch (train):   0%|          | 0/234 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (1/1) accuracy: 0.9724559187889099 | accuracy/std: 0.010148449210692417 | accuracy01: 0.9724559187889099 | accuracy01/std: 0.010148449210692417 | loss: 0.3903220295906067 | loss_ae: 0.1972823143005371 | loss_ae/mean: 0.1972823143005371 | loss_ae/std: 0.004739165075189417 | loss_ce: 0.0954778641462326 | loss_ce/mean: 0.0954778641462326 | loss_ce/std: 0.02652218401901059 | loss_kl: 9.756183624267578 | loss_kl/mean: 9.756183624267578 | loss_kl/std: 0.5093581846486388 | lr: 0.01 | momentum: 0.9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99b5a2774e854b0bb967c938964dad59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1/1 * Epoch (valid):   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid (1/1) accuracy: 0.98089998960495 | accuracy/std: 0.01270346459834009 | accuracy01: 0.98089998960495 | accuracy01/std: 0.01270346459834009 | loss: 0.33337920904159546 | loss_ae: 0.18005740642547607 | loss_ae/mean: 0.18005740642547607 | loss_ae/std: 0.007082436971827612 | loss_ce: 0.06260063499212265 | loss_ce/mean: 0.06260063499212265 | loss_ce/std: 0.034774403623831246 | loss_kl: 9.072114944458008 | loss_kl/mean: 9.072114944458008 | loss_kl/std: 0.21327841851032248 | lr: 0.01 | momentum: 0.9\n",
      "* Epoch (1/1) lr: 0.001 | momentum: 0.9\n",
      "Top best models:\n",
      "logs\\20210511-132334\\checkpoints/train.1.pth\t1.0000\n"
     ]
    }
   ],
   "source": [
    "runner.train(\n",
    "    engine=dl.DeviceEngine(device),\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    scheduler=scheduler,\n",
    "    loggers={\"tensorboard\": dl.TensorboardLogger(logdir=logdir)},\n",
    "    loaders={\"train\": train_data_loader, \"valid\": valid_data_loader},\n",
    "    callbacks=callbacks,\n",
    "    num_epochs=1,\n",
    "    logdir=logdir,\n",
    "    load_best_on_end=True,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare results with the usual VAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "locs, _ = model.encoder(test_data[\"image\"].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_transition(0, -3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how our model restore noised objects. The model aren't trained to restore, but it can do this very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(2, 6, figsize=(10, 4))\n",
    "    \n",
    "for k in range(0, 12):\n",
    "    image = test_data[\"image\"][k]\n",
    "    ax[k // 6][k % 6].imshow(image.squeeze().cpu().detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(2, 6, figsize=(10, 4))\n",
    "    \n",
    "for k in range(0, 12):\n",
    "    image = test_data[\"image\"][k]\n",
    "    noise = torch.rand(image.size()) # let's make some noise\n",
    "    ax[k // 6][k % 6].imshow((image + noise).squeeze().cpu().detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(2, 6, figsize=(10, 4))\n",
    "    \n",
    "for k in range(0, 12):\n",
    "    image = test_data[\"image\"][k]\n",
    "    noise = torch.rand(image.size()) * 0.4 # let's make some noise\n",
    "    point, _ = model.encoder((image + noise).unsqueeze(0).to(device)) # get noised objects vector representation from latent space\n",
    "    decoded = torch.sigmoid(model.decoder(point.unsqueeze(0).to(device)).squeeze()) # decode points\n",
    "    ax[k // 6][k % 6].imshow(decoded.cpu().detach().numpy()) # plot decoded\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, let's look at the latent space. We choose 2D plain space, so it's easy to plot the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {\"image\": [], \"loc\": [], \"target\": []}\n",
    "\n",
    "for pred in runner.predict_loader(loader=valid_data_loader):\n",
    "    # Put predicted loc and targets from pred into predictions\n",
    "    predictions[\"image\"].extend(o.reshape(28, 28) for o in pred[\"features\"].numpy())\n",
    "    predictions[\"loc\"].extend(i for i in pred[\"loc\"].cpu().numpy())\n",
    "    predictions[\"target\"].extend(i for i in pred[\"targets\"].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[\"x\"] = [o[0] for o in predictions[\"loc\"]]\n",
    "predictions[\"y\"] = [o[1] for o in predictions[\"loc\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "\n",
    "# Use sns.scatterplot to show points !\n",
    "_, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "sns.scatterplot(x=\"x\", y=\"y\", hue=\"target\", data=predictions, ax=ax, legend='full')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
