{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар 4. DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Memory\n",
    "\n",
    "Мы будем использовать воспроизведения опыта для тренировки нашей DQN. Replay Memory хранит\n",
    "переходы, которые наблюдает агент, что позволяет нам повторно использовать эти данные\n",
    "потом. Для уменьшения корреляций между состояниями и переходами мы будем выбирать случайные переходы из предыдущих игр."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем кортежи, в которых будем хранить наш опыт\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "# Создаем буфер, в котором будем хранить наш опыт\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        \n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "        \n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN algorithm\n",
    "\n",
    "Наша цель будет состоять в обучении политике, которая пытается максимизировать дисконтированные,\n",
    "накопительное вознаграждение\n",
    "$R_{t_0} = \\sum_{t=t_0}^{\\infty} \\gamma^{t - t_0} r_t$\n",
    "\n",
    "$Q^*: State \\times Action \\rightarrow \\mathbb{R}$\n",
    "\n",
    "\\begin{align}\\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a)\\end{align}\n",
    "\n",
    "Обновляем Q функцию исходя из того, что политика должна удовлетворять уравнению Буллмана\n",
    "\n",
    "\\begin{align}Q^{\\pi}(s, a) = r + \\gamma Q^{\\pi}(s', \\pi(s'))\\end{align}\n",
    "\n",
    "Разницу между правой и левой частями уравнения называем temporal difference error, $\\delta$:\n",
    "\n",
    "\\begin{align}\\delta = Q(s, a) - (r + \\gamma \\max_a Q(s', a))\\end{align}\n",
    "\n",
    "Именно эту ошибку мы и будем мимнимизировать"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создадим модель для апроксимации политики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        \n",
    "        \n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.head = nn.Linear(linear_input_size, outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Будем работать непосредственно с экраном игры, для этого объявим вспомогательные функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразование картинки в тензор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считывание положения каретки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cart_location(screen_width):\n",
    "\n",
    "    return   # центр каретки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считывание экрана игры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_screen():\n",
    "    # gym возвращает экран с размером 400x600x3 - многовато\n",
    "    screen = \n",
    "    \n",
    "    # Обрежем верх и низ картинки\n",
    "    \n",
    "    \n",
    "    # Центрируем картинку по каретке\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "        \n",
    "    # Обрезаем лишнюю часть, чтобы получить квадратную область\n",
    "    \n",
    "    \n",
    "    # Переводим картинку в тензор\n",
    "    \n",
    "    \n",
    "    # Сжимаем картинку\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "           interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_screen = get_screen()\n",
    "_, _, screen_height, screen_width = init_screen.shape\n",
    "\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объявляем две модели, одна отвечает за политику во время игры, другая для предсказания награды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "episode_durations = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выбор действия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    \n",
    "    \n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    \n",
    "    \n",
    "    # batch-array of Transitions to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Парсим transitions чтобы сформировать обучающую выборку\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Вычисляем Q(s_t, a) - Модель предсказывает Q(s_t), \n",
    "    # затем мы выбираем Q, соответствующие сделанному действию\n",
    "    \n",
    "\n",
    "    # Вычисляем V(s_{t+1}) для всех следующих состояний target_net(non_final_next_states)\n",
    "    # берем максимальной значение\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    \n",
    "    \n",
    "    # Вычисляем ожидаемые значения Q функции\n",
    "    \n",
    "\n",
    "    # Вычисляем Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Шаг оптимизации\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Основной цикл обучения\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_episodes = 10\n",
    "for i_episode in range(num_episodes):\n",
    "    \n",
    "    # Инициализация\n",
    "    env.reset()\n",
    "    \n",
    "    \n",
    "    for t in count():\n",
    "        \n",
    "        # Выбираем действие\n",
    "        action = select_action(state)\n",
    "        _, reward, done, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        # Наблюдаем новое состояние\n",
    "\n",
    "\n",
    "\n",
    "        # Сохраняем transition в буфер\n",
    "\n",
    "\n",
    "        # Делаем шаг оптимизации\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "            \n",
    "    # Обновляем target network, копирую веса из policy_net\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
