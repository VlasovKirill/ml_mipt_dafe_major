{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Simple CNN.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwRfuC6RDb1B"
      },
      "source": [
        "# Seminar 1. CV. Simple CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-23T05:35:57.001270Z",
          "start_time": "2021-04-23T05:35:55.472593Z"
        },
        "id": "cMKPD9TZdokn"
      },
      "source": [
        "from IPython import display\n",
        "import numpy as np\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-23T05:35:57.887472Z",
          "start_time": "2021-04-23T05:35:57.864444Z"
        },
        "id": "7gsxpRimFHF-"
      },
      "source": [
        "# fix all seeds\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sNYtrJ3YxV3"
      },
      "source": [
        "## Пайплайн обучения CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-23T05:36:02.255303Z",
          "start_time": "2021-04-23T05:35:59.651707Z"
        },
        "id": "_MKsdG6GdorV"
      },
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import seaborn as sns\n",
        "\n",
        "!pip install mnist\n",
        "import mnist\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "sns.set()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-23T05:36:07.386190Z",
          "start_time": "2021-04-23T05:36:07.218403Z"
        },
        "id": "eVJCnXOUR01g"
      },
      "source": [
        "images = mnist.train_images() / 255\n",
        "labels = mnist.train_labels()\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(images, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpKqvXNJR01h"
      },
      "source": [
        "### Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxsxdBZwR01h"
      },
      "source": [
        "class MNISTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "        pass\n",
        "    \n",
        "    def __len__(self):\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0iTMg7GR01h"
      },
      "source": [
        "train_dataset = MNISTDataset(X_train, y_train)\n",
        "valid_dataset = MNISTDataset(X_valid, y_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65szdGZcR01h"
      },
      "source": [
        "batch_size = 32\n",
        "n_workers = 0\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
        "                                                        shuffle=True, num_workers=n_workers)\n",
        "val_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size,\n",
        "                                                      shuffle=False, num_workers=n_workers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oBSm6QdR01i"
      },
      "source": [
        "### Check data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MC8tffzdR01i"
      },
      "source": [
        "img, label = next(iter(train_loader))\n",
        "print(img.shape, label.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGfv8X4aR01i"
      },
      "source": [
        "### Simple CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i58PsKu1R01i"
      },
      "source": [
        "class BasicConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, **kwargs):\n",
        "        pass \n",
        "\n",
        "    def forward(self, x):\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qF3qmKLLVDXP"
      },
      "source": [
        "block = BasicConv2d(in_channels=32, out_channels=64, kernel_size=(3, 3))\n",
        "\n",
        "x = torch.ones(1, 32, 28, 28)\n",
        "out = block(x)\n",
        "print(out.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCnAUREIR01j"
      },
      "source": [
        "class SimpleLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, conv=BasicConv2d, n=1, *args, **kwargs):\n",
        "        super(SimpleLayer, self).__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            conv(in_channels, out_channels, *args, **kwargs),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.Dropout2d(0.2),\n",
        "            nn.MaxPool2d((2, 2))\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.block(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0oYdyQhVZ6j"
      },
      "source": [
        "block = SimpleLayer(in_channels=32, out_channels=64, kernel_size=(3, 3))\n",
        "print(block)\n",
        "\n",
        "x = torch.ones(1, 32, 28, 28)\n",
        "out = block(x)\n",
        "print(out.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T18I3eUkR01j"
      },
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, in_channels=3, n_classes=5, blocks_sizes=(32, 64, 128), *args, **kwargs):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        \n",
        "        self.fc_dim = 128\n",
        "        \n",
        "        self.in_out_block_sizes = list(zip(blocks_sizes, blocks_sizes[1:]))\n",
        "        self.blocks = nn.ModuleList([\n",
        "            SimpleLayer(in_channels, blocks_sizes[0], kernel_size=3, *args, **kwargs),\n",
        "            *[SimpleLayer(in_channels, out_channels, kernel_size=3, *args, **kwargs)\n",
        "              for in_channels, out_channels in self.in_out_block_sizes]\n",
        "        ])\n",
        "        \n",
        "        self.fc = nn.Linear(self.fc_dim, n_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        \n",
        "        bs, dim, h, w = x.shape\n",
        "        x = x.view(bs, -1)\n",
        "        \n",
        "        out = self.fc(x)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQq6qargR01j"
      },
      "source": [
        "model = SimpleCNN(in_channels=1, n_classes=10)\n",
        "print(model)\n",
        "\n",
        "x = torch.ones(32, 1, 28, 28)\n",
        "out = model(x)\n",
        "print('Output shape:', out.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G26s-CUQSB2C"
      },
      "source": [
        "### Init model and criterion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzzR6bC4R01k"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iszfj5raR01k"
      },
      "source": [
        "### Train loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nwAo4jfaxry"
      },
      "source": [
        "def train(model, optimizer, n_epoch=20, batch_size=256, device=\"cpu\"):\n",
        "    train_logs = {\"Train Loss\": [0,], \"Steps\": [0,]}\n",
        "    valid_logs = {\"Valid Loss\": [0,], \"Valid Accuracy\": [0,], \"Steps\": [0,]}\n",
        "    step = 0\n",
        "    best_valid_loss = np.inf\n",
        "    best_model = None\n",
        "\n",
        "    for i in range(n_epoch):\n",
        "        for x_batch, y_batch in tqdm(train_loader, desc=f'train {i}/{n_epoch}'):\n",
        "            x_batch = x_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            with torch.set_grad_enabled(True):\n",
        "                # pass code\n",
        "                loss = None\n",
        "                pass \n",
        "            \n",
        "            step += 1\n",
        "            train_logs[\"Train Loss\"].append(loss.detach().item())\n",
        "            train_logs[\"Steps\"].append(step)\n",
        "\n",
        "        sum_loss = 0\n",
        "        sum_acc = 0\n",
        "        count_valid_steps = 0\n",
        "        with torch.no_grad():\n",
        "            for x_batch, y_batch in tqdm(val_loader, desc=f'val {i}/{n_epoch}'):\n",
        "                x_batch = x_batch.to(device)\n",
        "                y_batch = y_batch.to(device)\n",
        "\n",
        "                predictions = model(x_batch)\n",
        "                loss = criterion(predictions, y_batch)\n",
        "                sum_loss += loss.item()\n",
        "                sum_acc += accuracy_score(y_batch.cpu().numpy(), np.argmax(predictions.cpu().numpy(), axis=1))\n",
        "                count_valid_steps += 1\n",
        "\n",
        "            valid_logs[\"Valid Loss\"].append(sum_loss / count_valid_steps)\n",
        "            valid_logs[\"Valid Accuracy\"].append(sum_acc / count_valid_steps)\n",
        "            valid_logs[\"Steps\"].append(step)\n",
        "\n",
        "            if best_valid_loss > sum_loss / count_valid_steps:\n",
        "                best_valid_loss = sum_loss / count_valid_steps\n",
        "                best_model = deepcopy(model)\n",
        "\n",
        "    fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n",
        "    sns.lineplot(x=\"Steps\", y=\"Train Loss\", data=train_logs, ax=ax[0])\n",
        "    sns.lineplot(x=\"Steps\", y=\"Valid Loss\", data=valid_logs, ax=ax[1])\n",
        "    sns.lineplot(x=\"Steps\", y=\"Valid Accuracy\", data=valid_logs, ax=ax[2])\n",
        "    plt.plot()\n",
        "\n",
        "    return best_model, train_logs, valid_logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufy_8neBa3hG"
      },
      "source": [
        "net, _, _ = train(model, optimizer, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcOEa7lcjiyO"
      },
      "source": [
        "### Logging\n",
        "\n",
        "Logging systems:\n",
        "- [Tensorboard](https://pytorch.org/docs/stable/tensorboard.html)\n",
        "- [WandB](https://www.wandb.com/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QV1AxNFbfapY"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cgo_htQZfamn"
      },
      "source": [
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qWa59wKf5Jd"
      },
      "source": [
        "def train(model, optimizer, n_epoch=20, batch_size=256, device=\"cpu\"):\n",
        "    writer = SummaryWriter(Path(\"logs\") / datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "    step = 0\n",
        "    best_valid_loss = np.inf\n",
        "    best_model = None\n",
        "\n",
        "    model.to(device)\n",
        "    for i in range(n_epoch):\n",
        "        for x_batch, y_batch in tqdm(train_loader, desc=f'train {i}/{n_epoch}'):\n",
        "            x_batch = x_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            with torch.set_grad_enabled(True):\n",
        "                # pass code\n",
        "                pass\n",
        "            \n",
        "            step += 1\n",
        "            writer.add_scalar(\"Train Loss\", loss.detach().item(), step)\n",
        "\n",
        "        sum_loss = 0\n",
        "        sum_acc = 0\n",
        "        count_valid_steps = 0\n",
        "        with torch.no_grad():\n",
        "            for x_batch, y_batch in tqdm(val_loader, desc=f'val {i}/{n_epoch}'):\n",
        "                x_batch = x_batch.to(device)\n",
        "                y_batch = y_batch.to(device)\n",
        "\n",
        "                predictions = model(x_batch)\n",
        "                loss = criterion(predictions, y_batch)\n",
        "                sum_loss += loss.item()\n",
        "                sum_acc += accuracy_score(y_batch.cpu().numpy(), np.argmax(predictions.cpu().numpy(), axis=1))\n",
        "                count_valid_steps += 1\n",
        "\n",
        "            writer.add_scalar(\"Valid Loss\", sum_loss / count_valid_steps, step)\n",
        "            writer.add_scalar(\"Valid Accuracy\", sum_acc / count_valid_steps, step)\n",
        "\n",
        "            if best_valid_loss > sum_loss / count_valid_steps:\n",
        "                best_valid_loss = sum_loss / count_valid_steps\n",
        "                best_model = deepcopy(model)\n",
        "\n",
        "    return best_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qptL6E_Mf5G5",
        "scrolled": false
      },
      "source": [
        "net = train(model, optimizer, n_epoch=1, device=\"cuda:0\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuzkIqc0f5FG"
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7W1zyKnXFdxt"
      },
      "source": [
        "## Spoiler - train loop with [Catalyst](https://github.com/catalyst-team/catalyst)\n",
        "\n",
        "- [A comprehensive step-by-step guide to basic and advanced features](https://github.com/catalyst-team/catalyst#step-by-step-guide)\n",
        "- [Docs](https://catalyst-team.github.io/catalyst/)\n",
        "- [What is Runner?](https://catalyst-team.github.io/catalyst/api/core.html#runner)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKN0IXI2R01n"
      },
      "source": [
        "! pip install catalyst\n",
        "import catalyst\n",
        "catalyst.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gm2SIio7f5Cd",
        "scrolled": false
      },
      "source": [
        "import os\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from catalyst import dl, utils\n",
        "from catalyst.data.transforms import ToTensor\n",
        "from catalyst.contrib.datasets import MNIST\n",
        "\n",
        "model = nn.Sequential(nn.Flatten(), nn.Linear(28 * 28, 10))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.02)\n",
        "\n",
        "loaders = {\n",
        "    \"train\": DataLoader(\n",
        "        MNIST(os.getcwd(), train=True, download=True, transform=ToTensor()), batch_size=32\n",
        "    ),\n",
        "    \"valid\": DataLoader(\n",
        "        MNIST(os.getcwd(), train=False, download=True, transform=ToTensor()), batch_size=32\n",
        "    ),\n",
        "}\n",
        "\n",
        "runner = dl.SupervisedRunner(\n",
        "    input_key=\"features\", output_key=\"logits\", target_key=\"targets\", loss_key=\"loss\"\n",
        ")\n",
        "# model training\n",
        "runner.train(\n",
        "    model=model,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    loaders=loaders,\n",
        "    num_epochs=1,\n",
        "    callbacks=[\n",
        "        dl.AccuracyCallback(input_key=\"logits\", target_key=\"targets\", topk_args=(1, 3, 5)),\n",
        "        dl.PrecisionRecallF1SupportCallback(\n",
        "            input_key=\"logits\", target_key=\"targets\", num_classes=10\n",
        "        ),\n",
        "        dl.AUCCallback(input_key=\"logits\", target_key=\"targets\"),\n",
        "        # catalyst[ml] required ``pip install catalyst[ml]``\n",
        "        dl.ConfusionMatrixCallback(input_key=\"logits\", target_key=\"targets\", num_classes=10),\n",
        "    ],\n",
        "    logdir=Path(\"logs\") / datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
        "    valid_loader=\"valid\",\n",
        "    valid_metric=\"loss\",\n",
        "    minimize_valid_metric=True,\n",
        "    verbose=True,\n",
        "    load_best_on_end=True,\n",
        ")\n",
        "# model inference\n",
        "for prediction in runner.predict_loader(loader=loaders[\"valid\"]):\n",
        "    assert prediction[\"logits\"].detach().cpu().numpy().shape[-1] == 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "E2_QpeMNR01n"
      },
      "source": [
        "# Do you forget about logging?\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6vaVGaAR01o"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}